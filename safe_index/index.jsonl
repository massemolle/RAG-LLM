{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 0, "text": "Embedding and Indexing Methods for RAG Application\nThis document aims at briefly introducing the different indexing and embedding methods that were\nused in the RAG project. Embedding refers to representing text (words, sentences, or documents) as\ndense numerical vectors that capture semantic meaning, allowing for similarity comparisons based on\ncontext. Indexing, on the other hand, is the process of organizing and storing documents in a way that\nenables efficient lookup, typically using keywords or token positions, as in inverted indexes. Not all\nthese methods were tested in the framework, but enough code and documentation should be present\nto allow for their inclusion in the final RAG.\n1 Word2Vec : Continuous Bag of Word and Skip-Gram\nWord2Vec is a technique in natural language processing"}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 1, "text": " the final RAG.\n1 Word2Vec : Continuous Bag of Word and Skip-Gram\nWord2Vec is a technique in natural language processing used to learn distributed representations of\nwords, also known as word embeddings. Instead of representing words as sparse one-hot vectors,\nWord2Vec maps each word to a dense, continuous vector in a lower-dimensional space where seman-\ntically similar words are placed closer together. This approach captures both syntactic and semantic\nrelationships between words and is foundational in modern NLP.\nWord2Vec consists of two main model architectures: Continuous Bag of Words (CBOW) and\nSkip-gram. Both models are trained using a shallow neural network with one hidden layer, but they\napproach the learning task in opposite directions. The CBOW model predicts a target word based\n"}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 2, "text": "e hidden layer, but they\napproach the learning task in opposite directions. The CBOW model predicts a target word based\non its surrounding context words. It does this by representing each context word as a one-hot vector,\nprojecting them into an embedding space, averaging the vectors, and feeding the result into a softmax\nlayer to predict the most likely center word. On the other hand, the Skip-gram model reverses this\ntask by predicting context words based on a single center word. For each context word, the model\ncreates a separate training pair, treating it as an independent prediction problem\nFigure 1: Word2Vec methods, and difference between CBOW and Skip-Gram\n2 Okapi BM25\nOkapi BM25 is a ranking function used to estimate the relevance of documents to a given search query.\nIt is based "}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 3, "text": " BM25\nOkapi BM25 is a ranking function used to estimate the relevance of documents to a given search query.\nIt is based on the bag-of-words model, meaning it treats a document as a collection of individual\nterms, ignoring grammar and word order. BM25 improves upon traditional term-frequency approaches\nby introducing a probabilistic framework and tuning parameters that balance term frequency and\ndocument length. The main idea behind BM25 is that if a term appears more often in a document, it\nis more likely to be relevant—but with diminishing returns. That is, seeing a term 10 times is more\ninformative than seeing it once, but seeing it 100 times isn’t 10 times more useful than seeing it 10\n1times. BM25 applies a saturation function to the raw term frequency to account for this. The BM25\nsco"}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 4, "text": "than seeing it 10\n1times. BM25 applies a saturation function to the raw term frequency to account for this. The BM25\nscore of a document D regarding a query Q is calculated using the following formula:\nscore(D, Q) =\nnX\ni=1\nIDF (qi) f(qi, D) · (k1 + 1)\nf(qi, D) + k1 · (1 − b + b · |D|\navgdl\nwhere f(qi, D) is the frequency of the term qi in the document D, |D| the lengh of the document D\n(in terms of worlds) and avgdl is the average document lengh in the collection. The variables k1 and\nb are hyperparameters, usually with b = 0.75 and k1 ∈ [1.2 ; 2.0].\nThe inverse document frequency component (IDF) in BM25 ensures that common terms, which ap-\npear in many documents, are given less weight than rare, more discriminative terms. Unlike TF-IDF,\nwhich uses a logarithmic IDF, BM25 uses a more robus"}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 5, "text": "en less weight than rare, more discriminative terms. Unlike TF-IDF,\nwhich uses a logarithmic IDF, BM25 uses a more robust form that also accounts for term saturation.\nBM25 is effective because it balances three important factors: how often a query term appears in a\ndocument, how common or rare the term is across the whole collection, and how long the document\nis relative to the average. This balance allows it to rank documents in a way that aligns well with\nhuman judgments of relevance, making it a strong baseline for information retrieval tasks, such as\nsearch engines and recommendation systems. However this method is reliant on exact term matching,\nmeaning that synonyms and spelling mistakes will be considered as different terms.\n3 BERT : Bidirectional Encoder Representations from Trans-"}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 6, "text": " and spelling mistakes will be considered as different terms.\n3 BERT : Bidirectional Encoder Representations from Trans-\nformers\nBERT is a language representation model that processes text using the Transformer encoder archi-\ntecture. It operates by taking a sentence or a pair of sentences as input and representing every word\nin that input as a contextualized vector. This means each word is embedded based not only on its\nindividual meaning but also on the words that appear around it—both before and after—thanks to\nthe model’s bidirectional attention mechanism.\nThe input to BERT starts with tokenization. Each word or subword is broken down into tokens\nusing a method called WordPiece. Special tokens are added: [CLS] is placed at the beginning of\nthe sequence to represent the overall meaning "}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 7, "text": " WordPiece. Special tokens are added: [CLS] is placed at the beginning of\nthe sequence to represent the overall meaning of the sentence or pair of sentences, and [SEP] is used\nto separate two sentences. BERT then converts these tokens into embeddings, which include token\nembeddings, positional embeddings (to preserve word order), and segment embeddings (to differen-\ntiate between two input sentences, if applicable). These combined embeddings are passed into the\nTransformer encoder.\nWithin the encoder, BERT uses self-attention to model relationships between all tokens. In each\nlayer, every token attends to every other token in the sequence, and learns how much attention it\nshould pay to each of them based on their content. This allows BERT to capture dependencies and\nnuances across entire s"}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 8, "text": "\nshould pay to each of them based on their content. This allows BERT to capture dependencies and\nnuances across entire sentences, not just local context. Multiple layers of self-attention and feedforward\noperations refine these representations, resulting in a final embedding for each token that reflects its\nfull linguistic context.\nFigure 2: BERT embedding\nBERT is pretrained using two tasks. The first is masked language modeling, where some tokens in\nthe input are randomly replaced with a [MASK] token, and the model is trained to predict the original\ntokens. This forces the model to learn from both the left and right context of each masked word. The\n2second task is next sentence prediction, where the model receives two sentence segments and learns\nto predict whether the second follows the "}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 9, "text": "entence prediction, where the model receives two sentence segments and learns\nto predict whether the second follows the first in the original text. These objectives guide BERT to\nunderstand both word-level and sentence-level relationships.\nOnce pretrained, BERT can be adapted to downstream tasks by adding a small neural layer on top\nof the model and fine-tuning it using labeled data. For example, for a classification task, the [CLS]\ntoken’s final embedding can be passed to a softmax layer to produce class probabilities. For token-level\ntasks like named entity recognition, each token’s embedding can be used for individual classification.\nOverall, BERT functions as a general-purpose language understanding model. Its key contribu-\ntion is the ability to represent words in context, using deep "}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 10, "text": "l-purpose language understanding model. Its key contribu-\ntion is the ability to represent words in context, using deep bidirectional processing and attention\nmechanisms to capture meaning across entire sequences.\n4 DPR : Dense Passage Retrieval\nDense Passage Retrieval (DPR) is a neural information retrieval method designed to improve search\nby moving beyond exact keyword matching. It uses dense vector representations, or embeddings, to\nencode both user queries and candidate passages, enabling retrieval based on semantic similarity rather\nthan surface-level word overlap. The core idea is to train two separate encoders—one for questions and\none for passages—usually based on a pre-trained transformer model such as BERT. These encoders\ntransform the input text into fixed-size vectors within t"}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 11, "text": "n a pre-trained transformer model such as BERT. These encoders\ntransform the input text into fixed-size vectors within the same embedding space.\nDuring training, DPR is typically optimized using a contrastive learning approach. For each ques-\ntion, the model is given a relevant (positive) passage and several irrelevant (negative) passages. It\nlearns to produce embeddings such that the dot product (or cosine similarity) between the question\nand its relevant passage is higher than with the negatives. After training, the passage encoder is used\nto encode a large corpus of documents into embeddings, which are stored in a vector index. At retrieval\ntime, the question encoder encodes a user’s query, and the system retrieves the most similar passages\nby comparing the query embedding to all docume"}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 12, "text": "ncodes a user’s query, and the system retrieves the most similar passages\nby comparing the query embedding to all document embeddings using nearest neighbor search.\nThis dense retrieval approach allows DPR to capture semantic meaning and retrieve relevant content\neven when the exact terms in the question don’t appear in the passage. As a result, DPR is especially\npowerful for open-domain question answering tasks, where it can find informative evidence from large\ntext collections that keyword-based systems like BM25 may miss.\n5 GloVe: Global Vectors for Word Representation\nGloVe (Global Vectors for Word Representation) is a word embedding method that learns dense vector\nrepresentations of words by leveraging global co-occurrence statistics from a large text corpus. The\nmethod is based on th"}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 13, "text": "presentations of words by leveraging global co-occurrence statistics from a large text corpus. The\nmethod is based on the idea that the meaning of a word can be inferred from the distribution of other\nwords it co-occurs with.\nFirst, a co-occurrence matrix X is constructed, where each entry Xij records how often word j\nappears in the context of word i. The goal is to learn word vectors wi and context word vectors ˜wj such\nthat their dot product, plus bias terms, approximates the logarithm of the observed co-occurrence:\nw⊤\ni ˜wj + bi + ˜bj ≈ log Xij.\nThe learning objective minimizes the squared difference between the left and right sides of this rela-\ntionship, weighted by a function f(Xij) that limits the influence of extremely frequent co-occurrences.\nThe cost function is:\nJ =\nVX\ni=1\nVX\nj="}
{"collection": "grid_ops", "doc": "Embeddings.pdf", "doc_sha": "b27ac9d873da7d2ccce7c862c75cce49104b4955daaed8fdc829c3697eb6b28c", "chunk": 14, "text": "a function f(Xij) that limits the influence of extremely frequent co-occurrences.\nThe cost function is:\nJ =\nVX\ni=1\nVX\nj=1\nf(Xij)\n\u0010\nw⊤\ni ˜wj + bi + ˜bj − log Xij\n\u00112\nwhere V is the vocabulary size, bi and ˜bj are bias terms, and f(Xij) is the weighting function\ndefined as:\nf(x) =\n(\u0010\nx\nxmax\n\u0011α\nif x < xmax\n1 otherwise\n3with typical values xmax ≈ 100 and α ≈ 0.75. This formulation encourages words that appear in\nsimilar contexts to have similar vector representations, capturing both semantic and syntactic rela-\ntionships.\n4"}
{"collection": "grid_ops", "doc": "embeddings101.txt", "doc_sha": "caa3da4167b245efbd5c269d89e9e5b72817f3bb7d0dfb51a3d00e8c1e0cc1a6", "chunk": 0, "text": "Embeddings map tokens or documents to vectors in R^d.\nCosine similarity is commonly used to compare embeddings.\nL2-normalization helps when using dot-product scoring."}
